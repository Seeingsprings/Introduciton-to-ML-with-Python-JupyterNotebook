








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


X = np.array([[0, 1, 0, 1], 
              [1, 0, 1, 1], 
              [0, 0, 0, 1], 
              [1, 0, 1, 0]]) 
y = np.array([0, 1, 0, 1])






counts = {} 
for label in np.unique(y): 
    # 对每个类别进行遍历 
    # 计算（求和）每个特征中1的个数 
    counts[label] = X[y == label].sum(axis=0) 
print("Feature counts:\n{}".format(counts))








import sys
print("Using Python from:", sys.executable)


import mglearn
mglearn.plots.plot_animal_tree()








from sklearn.tree import DecisionTreeClassifier 
cancer = load_breast_cancer() 
X_train, X_test, y_train, y_test = train_test_split( 
    cancer.data, cancer.target, stratify=cancer.target, random_state=42) 
tree = DecisionTreeClassifier(random_state=0) 
tree.fit(X_train, y_train) 
print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))





tree = DecisionTreeClassifier(max_depth=4, random_state=0) 
tree.fit(X_train, y_train) 
print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))





from sklearn.tree import export_graphviz 
# 如果你想用全部 30 个特征
X = cancer.data  # 全部30个特征
y = cancer.target

tree.fit(X, y)   # 模型现在知道有30个特征

# 导出树
export_graphviz(tree,
                out_file="tree.dot",
                class_names=["malignant","benign"],
                feature_names=cancer.feature_names,  # 现在可以用了，因为确实是30个
                impurity=False,
                filled=True)


import graphviz 
with open("tree.dot") as f: 
    dot_graph = f.read() 
graphviz.Source(dot_graph)





print("Feature importances:\n{}".format(tree.feature_importances_))


def plot_feature_importances_cancer(model): 
    n_features = cancer.data.shape[1] 
    plt.barh(range(n_features), model.feature_importances_, align='center') 
    plt.yticks(np.arange(n_features), cancer.feature_names) 
    plt.xlabel("Feature importance") 
    plt.ylabel("Feature") 
plot_feature_importances_cancer(tree)





tree = mglearn.plots.plot_tree_not_monotone() 
display(tree)





import os
ram_prices = pd.read_csv(os.path.join(mglearn.datasets.DATA_PATH, "ram_price.csv"))

plt.semilogy(ram_prices.date, ram_prices.price)
plt.xlabel("Year")
plt.ylabel("Price in $/Mbyte")





from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# 假设 ram_prices 是一个 DataFrame，包含 'date' 和 'price' 两列

# 分割训练集和测试集
data_train = ram_prices[ram_prices.date < 2000]
data_test = ram_prices[ram_prices.date >= 2000]

# 准备训练数据：特征 X 是 date，目标 y 是 log(price)
X_train = data_train.date.values.reshape(-1, 1)      # ✅ 改这里！
y_train = np.log(data_train.price)

# 训练模型
tree = DecisionTreeRegressor().fit(X_train, y_train)
linear_reg = LinearRegression().fit(X_train, y_train)

# 对所有年份做预测（从最早到最晚）
X_all = ram_prices.date.values.reshape(-1, 1)         # ✅ 也改这里！

pred_tree = tree.predict(X_all)
pred_lr = linear_reg.predict(X_all)

# 逆变换：把 log(price) 变回原始价格
price_tree = np.exp(pred_tree)
price_lr = np.exp(pred_lr)

plt.figure(figsize=(12, 6))
plt.semilogy(ram_prices.date, ram_prices.price, label="True Data", color="black")
plt.semilogy(ram_prices.date, price_tree, label="Decision Tree", linestyle='--')
plt.semilogy(ram_prices.date, price_lr, label="Linear Regression", linestyle='-.')
plt.xlabel("Year")
plt.ylabel("Price")
plt.legend()
plt.show()




















fig, axes = plt.subplots(2, 3, figsize=(20, 10)) 
for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)): 
    ax.set_title("Tree {}".format(i)) 
    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax) 
mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1], 
                                alpha=.4) 
axes[-1, -1].set_title("Random Forest") 
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)





X_train, X_test, y_train, y_test = train_test_split( 
    cancer.data, cancer.target, random_state=0) 
forest = RandomForestClassifier(n_estimators=100, random_state=0) 
forest.fit(X_train, y_train) 
print("Accuracy on training set: {:.3f}".format(forest.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, y_test)))





plot_feature_importances_cancer(forest)











from sklearn.ensemble import GradientBoostingClassifier 
X_train, X_test, y_train, y_test = train_test_split( 
    cancer.data, cancer.target, random_state=0) 
gbrt = GradientBoostingClassifier(random_state=0) 
gbrt.fit(X_train, y_train) 
print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test)))





# 强预剪枝
gbrt = GradientBoostingClassifier(random_state=0, max_depth=1) 
gbrt.fit(X_train, y_train) 
print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test)))


# 降低learning rate
gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01) 
gbrt.fit(X_train, y_train)
print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test)))





gbrt = GradientBoostingClassifier(random_state=0, max_depth=1) 
gbrt.fit(X_train, y_train) 
plot_feature_importances_cancer(gbrt)






