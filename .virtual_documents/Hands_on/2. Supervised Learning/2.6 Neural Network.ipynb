





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import mglearn
from sklearn.datasets import make_blobs
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


display(mglearn.plots.plot_logistic_regression_graph())





display(mglearn.plots.plot_single_hidden_layer_graph())





line = np.linspace(-3, 3, 100) 
plt.plot(line, np.tanh(line), label="tanh") 
plt.plot(line, np.maximum(line, 0), label="relu") 
plt.legend(loc="best") 
plt.xlabel("x") 
plt.ylabel("relu(x), tanh(x)")





mglearn.plots.plot_two_hidden_layer_graph()





from sklearn.neural_network import MLPClassifier 
from sklearn.datasets import make_moons 
X, y = make_moons(n_samples=100, noise=0.25, random_state=3) 
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, 
                                                    random_state=42) 
mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train) 
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3) 
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")





mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10]) 
mlp.fit(X_train, y_train) 
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3) 
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")





# 使用2个隐层，每个包含10个单元，relu非线性
mlp = MLPClassifier(solver='lbfgs', random_state=0, 
                    hidden_layer_sizes=[10, 10]) 
mlp.fit(X_train, y_train) 
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3) 
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")





# 使用2个隐层，每个包含10个单元，这次使用tanh非线性 
mlp = MLPClassifier(solver='lbfgs', activation='tanh', 
                    random_state=0, hidden_layer_sizes=[10, 10]) 
mlp.fit(X_train, y_train) 
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3) 
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")





fig, axes = plt.subplots(2, 4, figsize=(20, 8)) 
for axx, n_hidden_nodes in zip(axes, [10, 100]): 
    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]): 
        mlp = MLPClassifier(solver='lbfgs', random_state=0, 
                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes], 
                            alpha=alpha) 
        mlp.fit(X_train, y_train) 
        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax) 
        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax) 
        ax.set_title("n_hidden=[{}, {}]\nalpha={:.4f}".format( 
                      n_hidden_nodes, n_hidden_nodes, alpha))








fig, axes = plt.subplots(2, 4, figsize=(20, 8)) 
for i, ax in enumerate(axes.ravel()): 
    mlp = MLPClassifier(solver='lbfgs', random_state=i, 
                        hidden_layer_sizes=[100, 100]) 
    mlp.fit(X_train, y_train)
    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax) 
    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)





from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 2: Load the Data
data = load_breast_cancer()
X = data.data    # Features (输入): shape (569, 30)
y = data.target  # Labels (输出): 0 = benign, 1 = malignant

print("Feature shape:", X.shape)  # Should be (569, 30)
print("Label shape:", y.shape)    # Should be (569,)





# Step 3: Split into Train and Test Sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,         # Use 20% for testing
    random_state=42        # For reproducibility (可重复性)
)





# Step 4: Standardize the Features (Very Important!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit on train
X_test_scaled = scaler.transform(X_test)        # Only transform test!





# Step 5: Build and Train the MLPClassifier
mlp = MLPClassifier(
    hidden_layer_sizes=(100,),      # One hidden layer with 100 neurons
    activation='relu',              # ReLU 激活函数
    solver='adam',                  # Adam optimizer (推荐！)
    max_iter=500,                   # Max epochs (迭代次数)
    alpha=0.001,                    # L2 regularization (正则化强度)
    random_state=42,                # Reproducible results
    verbose=False                   # No printing during training
)

# Train the model
mlp.fit(X_train_scaled, y_train)





#  Step 6: Make Predictions and Evaluate
# Predict on test set
y_pred = mlp.predict(X_test_scaled)

# Compute accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {acc:.4f}")  # e.g., 0.97 or 97%


# create a report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))





import matplotlib.pyplot as plt

# Train again, but this time ask for loss curve
mlp = MLPClassifier(
    hidden_layer_sizes=(100,),
    activation='relu',
    solver='adam',
    max_iter=500,
    alpha=0.001,
    random_state=42,
    verbose=False,
    early_stopping=False  # Keep all epochs for plotting
)

# Fit and record loss
mlp.fit(X_train_scaled, y_train)

# Plot loss vs iteration
plt.figure(figsize=(8, 5))
plt.plot(mlp.loss_curve_, label='Training Loss', color='blue')
plt.title('MLP: Training Loss Over Iterations')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()
plt.show()





# Add early stopping
mlp_es = MLPClassifier(
    hidden_layer_sizes=(100,),
    activation='relu',
    solver='adam',
    max_iter=500,
    alpha=0.001,
    random_state=42,
    early_stopping=True,           # ← Turn ON early stopping
    validation_fraction=0.1,       # Use 10% of training data as validation
    n_iter_no_change=10            # Stop if no improvement in 10 iterations
)

# Train
mlp_es.fit(X_train_scaled, y_train)

# Compare: how many iterations did it actually use?
print("Actual iterations used:", len(mlp_es.loss_curve_))  # Maybe only 150 instead of 500!


plt.figure(figsize=(9, 6))
plt.plot(mlp.loss_curve_, label='Without Early Stopping', color='blue')
plt.plot(mlp_es.loss_curve_, label='With Early Stopping', color='red', linestyle='--')
plt.title('Compare Training: With vs Without Early Stopping')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()












