








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


X = np.array([[0, 1, 0, 1], 
              [1, 0, 1, 1], 
              [0, 0, 0, 1], 
              [1, 0, 1, 0]]) 
y = np.array([0, 1, 0, 1])






counts = {} 
for label in np.unique(y): 
    # ÂØπÊØè‰∏™Á±ªÂà´ËøõË°åÈÅçÂéÜ 
    # ËÆ°ÁÆóÔºàÊ±ÇÂíåÔºâÊØè‰∏™ÁâπÂæÅ‰∏≠1ÁöÑ‰∏™Êï∞ 
    counts[label] = X[y == label].sum(axis=0) 
print("Feature counts:\n{}".format(counts))








import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report


# Create a simple dataset
data = {
    'email': [
        'Free money now!',                     # spam
        'Win cash fast!',                      # spam
        'Call you later, meeting at 5?',       # ham
        'Congrats! You won $1000!',            # spam
        'Can we schedule a call?',             # ham
        'URGENT: Claim your prize!',           # spam
        'Thanks for the document.',            # ham
        'Make money fast with no effort.'      # spam
    ],
    'label': ['spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam']
}

df = pd.DataFrame(data)
print(df)





# Initialize the vectorizer
vectorizer = CountVectorizer()

# Fit and transform the emails into feature vectors
X = vectorizer.fit_transform(df['email'])

# Labels (target)
y = df['label']





X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)








# Create and train the classifier
model = MultinomialNB()
model.fit(X_train, y_train)





# Predict on test set
y_pred = model.predict(X_test)

# Show results
print("Predicted labels:", y_pred)
print("Actual labels:   ", y_test.values)





acc = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {acc:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))





new_email = ["Congratulations! You've won a free iPhone!"]

# Must use the same vectorizer (trained on original vocabulary)
new_email_vec = vectorizer.transform(new_email)

# Predict
prediction = model.predict(new_email_vec)
probability = model.predict_proba(new_email_vec)

print("Prediction:", prediction[0])
print("Probability [ham, spam]:", probability[0])





import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix
import re
from nltk.corpus import stopwords


import nltk
nltk.download('stopwords')





# Load dataset, specify encoding because of special characters
df = pd.read_csv('spam.csv', encoding='latin-1')

# Look at first 5 rows
print(df.head())





# Fix broken column names
df.columns = ['label', 'message', 'unknown1', 'unknown2', 'unknown3']

# Keep only the first two columns
df = df[['label', 'message']]

# ‚ö†Ô∏è CRITICAL STEP: Remove any rows where message is NaN or empty
df.dropna(subset=['message'], inplace=True)  # Remove missing messages
df = df.reset_index(drop=True)               # Reset index








# Step 5: Preprocess text
def preprocess_text(text):
    text = text.lower()
    # Only remove non-letters, but keep $ and !
    text = re.sub(r'[^a-zA-Z\s\$!]', '', text)
    return text.strip()

# Apply preprocessing to all messages
df['clean_message'] = df['message'].apply(preprocess_text)

# Show original vs cleaned
print("\nOriginal vs Cleaned:")
print(df[['message', 'clean_message']].head())





from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(
    max_features=5000,
    stop_words=None,       # ‚Üê Let‚Äôs NOT remove stopwords automatically
    ngram_range=(1, 2)     # ‚Üê Add bigrams like "won money", "click here"
)
X = vectorizer.fit_transform(df['clean_message'])

# Labels (target): spam or ham
y = df['label']





# Split: 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training samples:", X_train.shape[0])
print("Testing samples: ", X_test.shape[0])





# Create and train the classifier
model = MultinomialNB()
model.fit(X_train, y_train)

print("Model trained successfully! üéâ")





# Predict on test set
y_pred = model.predict(X_test)

# Predict probabilities (optional)
y_prob = model.predict_proba(X_test)  # [P(ham), P(spam)]


# Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}")

# Precision, Recall, F1-Score
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))





import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Get predictions
y_pred = model.predict(X_test)

# Create confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=['ham', 'spam'])

# Plot heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()





new_msg = ["Congratulations! You've won $1000! Click here now!"]

# Must clean and vectorize the same way!
clean_new = preprocess_text(new_msg[0])
vec_new = vectorizer.transform([clean_new])

pred = model.predict(vec_new)
prob = model.predict_proba(vec_new)

print("Message:", new_msg[0])
print("Prediction:", pred[0])
print("Spam probability: {:.2f}%".format(prob[0][1] * 100))
