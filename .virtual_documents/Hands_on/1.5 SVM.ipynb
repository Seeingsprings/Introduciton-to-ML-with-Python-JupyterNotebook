





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import mglearn
from sklearn.datasets import make_blobs
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix





%matplotlib inline  
# 在 Jupyter 中显示图像

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons      # 生成“月亮形”数据
from mpl_toolkits.mplot3d import Axes3D     # 3D 绘图支持
from sklearn.svm import SVC                 # SVM 模型

# 生成 40 个点，像两个月亮，加一点噪声让它更真实
X, y = make_moons(n_samples=40, noise=0.1, random_state=8)

# 查看一下数据长什么样
print("X shape:", X.shape)  # 应该是 (40, 2) —— 40 个点，每个有 2 个特征
print("First 5 points:\n", X[:5])
print("Labels:", y[:10])   # y 是 0 或 1





# 在二维空间中画出原始数据
plt.figure(figsize=(6, 4))
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0', edgecolors='k')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1', edgecolors='k')

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.legend()
plt.title("Original 2D Data (Not Linearly Separable)")
plt.grid(True)
plt.show()





# 创建新特征：x1 的平方
X_new = np.hstack([X, X[:, 1:] ** 2])

# 检查新数据形状
print("X_new shape:", X_new.shape)  # 应该是 (40, 3)
print("First few rows of X_new:\n", X_new[:3])





fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')  # 创建 3D 坐标系

# 分类绘制
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2],
           c='blue', s=80, label='Class 0', depthshade=False)

ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2],
           c='red', marker='^', s=80, label='Class 1', depthshade=False)

# 设置坐标轴标签
ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature 1 ** 2")

# 调整视角，看得更清楚
ax.view_init(elev=20, azim=30)

# 显示图例
plt.legend()
plt.title("3D View: After Adding Feature1 Squared")
plt.show()





# 使用线性 SVM（因为我们已经做了特征工程）
svm = SVC(kernel='linear', C=1000)  # C 很大表示不允许误分类（用于教学）
svm.fit(X_new, y)





# 创建网格点用于画平面
xx, yy = np.meshgrid(np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 10),
                     np.linspace(X_new[:, 1].min(), X_new[:, 1].max(), 10))
zz = (-svm.intercept_[0] - svm.coef_[0][0] * xx - svm.coef_[0][1] * yy) / svm.coef_[0][2]

# 画图
fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')

# 重新画点
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='blue', s=80, label='Class 0')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='red', marker='^', s=80, label='Class 1')

# 画决策平面
ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', linewidth=0)

ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature 1 ** 2")
ax.view_init(elev=20, azim=30)
plt.legend()
plt.title("SVM Decision Plane in 3D Space")
plt.show()





# 生成干净数据（低噪音）
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 添加新特征
X_new = np.hstack([X, X[:, 1:] ** 2])

# 训练 SVM（高惩罚）
svm = SVC(kernel='linear', C=1e6)
svm.fit(X_new, y)

# 画图
fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')

mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='blue', s=80, label='Class 0')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='red', marker='^', s=80, label='Class 1')

# 决策平面
xx, yy = np.meshgrid(np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 10),
                     np.linspace(X_new[:, 1].min(), X_new[:, 1].max(), 10))
zz = (-svm.intercept_[0] - svm.coef_[0][0] * xx - svm.coef_[0][1] * yy) / svm.coef_[0][2]
ax.plot_surface(xx, yy, zz, alpha=0.5, color='green')

ax.set_xlabel("Feature 0"); ax.set_ylabel("Feature 1"); ax.set_zlabel("Feature 1 ** 2")
ax.view_init(elev=20, azim=30)
plt.legend()
plt.title("Perfectly Separable in 3D Space")
plt.show()





%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.svm import SVC

# 1. 生成数据
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 2. 添加新特征：feature1 的平方
X_new = np.hstack([X, X[:, 1:] ** 2])

# 3. 训练 SVM
svm = SVC(kernel='linear', C=1e6)
svm.fit(X_new, y)

# 4. 在 2D 网格上预测（这才是正确的做法）
xx, yy = np.meshgrid(np.linspace(-2, 4, 200),
                     np.linspace(-1, 1.5, 200))
grid_points = np.c_[xx.ravel(), yy.ravel()]
grid_points_new = np.hstack([grid_points, grid_points[:, 1:] ** 2])
Z = svm.predict(grid_points_new).reshape(xx.shape)

# 5. 画图
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu_r)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1')
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linewidths=2)

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("True Nonlinear Decision Boundary (via Prediction)")
plt.legend()
plt.grid(True)
plt.show()





%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.svm import SVC

# 1. 生成数据
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 2. 添加新特征：feature1 的平方
X_new = np.hstack([X, X[:, 1:] ** 2])

# 3. 使用 RBF 核（非线性）
svm = SVC(kernel='rbf', C=1e6, gamma='scale')  # 👈 改为 rbf
svm.fit(X_new, y)

# 4. 在网格上预测
xx, yy = np.meshgrid(np.linspace(-2, 4, 200),
                     np.linspace(-1, 1.5, 200))
grid_points = np.c_[xx.ravel(), yy.ravel()]
grid_points_new = np.hstack([grid_points, grid_points[:, 1:] ** 2])
Z = svm.predict(grid_points_new).reshape(xx.shape)

# 5. 画图
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu_r)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1')
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linewidths=2)

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("Nonlinear Decision Boundary (RBF Kernel)")
plt.legend()
plt.grid(True)
plt.show()








from sklearn.svm import SVC 
X, y = mglearn.tools.make_handcrafted_dataset() 
svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y) 
mglearn.plots.plot_2d_separator(svm, X, eps=.5) 
mglearn.discrete_scatter(X[:, 0], X[:, 1], y) 
# 画出支持向量 
sv = svm.support_vectors_ 
# 支持向量的类别标签由dual_coef_的正负号给出 
sv_labels = svm.dual_coef_.ravel() > 0 
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")






fig, axes = plt.subplots(3, 3, figsize=(15, 10)) 
for ax, C in zip(axes, [-1, 0, 3]): 
    for a, gamma in zip(ax, range(-1, 2)): 
        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a) 
axes[0, 0].legend(["class 0", "class 1", "sv class 0", "sv class 1"], 
                  ncol=4, loc=(.9, 1.2))





from sklearn.datasets import load_breast_cancer
X_train, X_test, y_train, y_test = train_test_split( 
    cancer.data, cancer.target, random_state=0) 
svc = SVC() 
svc.fit(X_train, y_train) 
print("Accuracy on training set: {:.2f}".format(svc.score(X_train, y_train))) 
print("Accuracy on test set: {:.2f}".format(svc.score(X_test, y_test)))





plt.plot(X_train.min(axis=0), 'o', label="min") 
plt.plot(X_train.max(axis=0), '^', label="max") 
plt.legend(loc=4) 
plt.xlabel("Feature index") 
plt.ylabel("Feature magnitude") 
plt.yscale("log")





# 计算训练集中每个特征的最小值 
min_on_training = X_train.min(axis=0) 
# 计算训练集中每个特征的范围（最大值-最小值） 
range_on_training = (X_train - min_on_training).max(axis=0) 
# 减去最小值，然后除以范围 
# 这样每个特征都是min=0和max=1 
X_train_scaled = (X_train - min_on_training) / range_on_training 
print("Minimum for each feature\n{}".format(X_train_scaled.min(axis=0))) 
print("Maximum for each feature\n {}".format(X_train_scaled.max(axis=0)))





# 利用训练集的最小值和范围对测试集做相同的变换（详见第3章） 
X_test_scaled = (X_test - min_on_training) / range_on_training


svc = SVC() 
svc.fit(X_train_scaled, y_train) 
print("Accuracy on training set: {:.3f}".format( 
    svc.score(X_train_scaled, y_train))) 
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))





svc = SVC(C=1000) 
svc.fit(X_train_scaled, y_train) 
print("Accuracy on training set: {:.3f}".format( 
    svc.score(X_train_scaled, y_train))) 
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))



