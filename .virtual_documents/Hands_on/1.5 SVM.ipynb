





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import mglearn
from sklearn.datasets import make_blobs
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix





%matplotlib inline  
# åœ¨ Jupyter ä¸­æ˜¾ç¤ºå›¾åƒ

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons      # ç”Ÿæˆâ€œæœˆäº®å½¢â€æ•°æ®
from mpl_toolkits.mplot3d import Axes3D     # 3D ç»˜å›¾æ”¯æŒ
from sklearn.svm import SVC                 # SVM æ¨¡å‹

# ç”Ÿæˆ 40 ä¸ªç‚¹ï¼Œåƒä¸¤ä¸ªæœˆäº®ï¼ŒåŠ ä¸€ç‚¹å™ªå£°è®©å®ƒæ›´çœŸå®
X, y = make_moons(n_samples=40, noise=0.1, random_state=8)

# æŸ¥çœ‹ä¸€ä¸‹æ•°æ®é•¿ä»€ä¹ˆæ ·
print("X shape:", X.shape)  # åº”è¯¥æ˜¯ (40, 2) â€”â€” 40 ä¸ªç‚¹ï¼Œæ¯ä¸ªæœ‰ 2 ä¸ªç‰¹å¾
print("First 5 points:\n", X[:5])
print("Labels:", y[:10])   # y æ˜¯ 0 æˆ– 1





# åœ¨äºŒç»´ç©ºé—´ä¸­ç”»å‡ºåŸå§‹æ•°æ®
plt.figure(figsize=(6, 4))
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0', edgecolors='k')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1', edgecolors='k')

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.legend()
plt.title("Original 2D Data (Not Linearly Separable)")
plt.grid(True)
plt.show()





# åˆ›å»ºæ–°ç‰¹å¾ï¼šx1 çš„å¹³æ–¹
X_new = np.hstack([X, X[:, 1:] ** 2])

# æ£€æŸ¥æ–°æ•°æ®å½¢çŠ¶
print("X_new shape:", X_new.shape)  # åº”è¯¥æ˜¯ (40, 3)
print("First few rows of X_new:\n", X_new[:3])





fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')  # åˆ›å»º 3D åæ ‡ç³»

# åˆ†ç±»ç»˜åˆ¶
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2],
           c='blue', s=80, label='Class 0', depthshade=False)

ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2],
           c='red', marker='^', s=80, label='Class 1', depthshade=False)

# è®¾ç½®åæ ‡è½´æ ‡ç­¾
ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature 1 ** 2")

# è°ƒæ•´è§†è§’ï¼Œçœ‹å¾—æ›´æ¸…æ¥š
ax.view_init(elev=20, azim=30)

# æ˜¾ç¤ºå›¾ä¾‹
plt.legend()
plt.title("3D View: After Adding Feature1 Squared")
plt.show()





# ä½¿ç”¨çº¿æ€§ SVMï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»åšäº†ç‰¹å¾å·¥ç¨‹ï¼‰
svm = SVC(kernel='linear', C=1000)  # C å¾ˆå¤§è¡¨ç¤ºä¸å…è®¸è¯¯åˆ†ç±»ï¼ˆç”¨äºæ•™å­¦ï¼‰
svm.fit(X_new, y)





# åˆ›å»ºç½‘æ ¼ç‚¹ç”¨äºç”»å¹³é¢
xx, yy = np.meshgrid(np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 10),
                     np.linspace(X_new[:, 1].min(), X_new[:, 1].max(), 10))
zz = (-svm.intercept_[0] - svm.coef_[0][0] * xx - svm.coef_[0][1] * yy) / svm.coef_[0][2]

# ç”»å›¾
fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')

# é‡æ–°ç”»ç‚¹
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='blue', s=80, label='Class 0')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='red', marker='^', s=80, label='Class 1')

# ç”»å†³ç­–å¹³é¢
ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', linewidth=0)

ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature 1 ** 2")
ax.view_init(elev=20, azim=30)
plt.legend()
plt.title("SVM Decision Plane in 3D Space")
plt.show()





# ç”Ÿæˆå¹²å‡€æ•°æ®ï¼ˆä½å™ªéŸ³ï¼‰
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# æ·»åŠ æ–°ç‰¹å¾
X_new = np.hstack([X, X[:, 1:] ** 2])

# è®­ç»ƒ SVMï¼ˆé«˜æƒ©ç½šï¼‰
svm = SVC(kernel='linear', C=1e6)
svm.fit(X_new, y)

# ç”»å›¾
fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')

mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='blue', s=80, label='Class 0')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='red', marker='^', s=80, label='Class 1')

# å†³ç­–å¹³é¢
xx, yy = np.meshgrid(np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 10),
                     np.linspace(X_new[:, 1].min(), X_new[:, 1].max(), 10))
zz = (-svm.intercept_[0] - svm.coef_[0][0] * xx - svm.coef_[0][1] * yy) / svm.coef_[0][2]
ax.plot_surface(xx, yy, zz, alpha=0.5, color='green')

ax.set_xlabel("Feature 0"); ax.set_ylabel("Feature 1"); ax.set_zlabel("Feature 1 ** 2")
ax.view_init(elev=20, azim=30)
plt.legend()
plt.title("Perfectly Separable in 3D Space")
plt.show()





%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.svm import SVC

# 1. ç”Ÿæˆæ•°æ®
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 2. æ·»åŠ æ–°ç‰¹å¾ï¼šfeature1 çš„å¹³æ–¹
X_new = np.hstack([X, X[:, 1:] ** 2])

# 3. è®­ç»ƒ SVM
svm = SVC(kernel='linear', C=1e6)
svm.fit(X_new, y)

# 4. åœ¨ 2D ç½‘æ ¼ä¸Šé¢„æµ‹ï¼ˆè¿™æ‰æ˜¯æ­£ç¡®çš„åšæ³•ï¼‰
xx, yy = np.meshgrid(np.linspace(-2, 4, 200),
                     np.linspace(-1, 1.5, 200))
grid_points = np.c_[xx.ravel(), yy.ravel()]
grid_points_new = np.hstack([grid_points, grid_points[:, 1:] ** 2])
Z = svm.predict(grid_points_new).reshape(xx.shape)

# 5. ç”»å›¾
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu_r)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1')
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linewidths=2)

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("True Nonlinear Decision Boundary (via Prediction)")
plt.legend()
plt.grid(True)
plt.show()





%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.svm import SVC

# 1. ç”Ÿæˆæ•°æ®
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 2. æ·»åŠ æ–°ç‰¹å¾ï¼šfeature1 çš„å¹³æ–¹
X_new = np.hstack([X, X[:, 1:] ** 2])

# 3. ä½¿ç”¨ RBF æ ¸ï¼ˆéçº¿æ€§ï¼‰
svm = SVC(kernel='rbf', C=1e6, gamma='scale')  # ğŸ‘ˆ æ”¹ä¸º rbf
svm.fit(X_new, y)

# 4. åœ¨ç½‘æ ¼ä¸Šé¢„æµ‹
xx, yy = np.meshgrid(np.linspace(-2, 4, 200),
                     np.linspace(-1, 1.5, 200))
grid_points = np.c_[xx.ravel(), yy.ravel()]
grid_points_new = np.hstack([grid_points, grid_points[:, 1:] ** 2])
Z = svm.predict(grid_points_new).reshape(xx.shape)

# 5. ç”»å›¾
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu_r)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1')
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linewidths=2)

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("Nonlinear Decision Boundary (RBF Kernel)")
plt.legend()
plt.grid(True)
plt.show()








from sklearn.svm import SVC 
X, y = mglearn.tools.make_handcrafted_dataset() 
svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y) 
mglearn.plots.plot_2d_separator(svm, X, eps=.5) 
mglearn.discrete_scatter(X[:, 0], X[:, 1], y) 
# ç”»å‡ºæ”¯æŒå‘é‡ 
sv = svm.support_vectors_ 
# æ”¯æŒå‘é‡çš„ç±»åˆ«æ ‡ç­¾ç”±dual_coef_çš„æ­£è´Ÿå·ç»™å‡º 
sv_labels = svm.dual_coef_.ravel() > 0 
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")






fig, axes = plt.subplots(3, 3, figsize=(15, 10)) 
for ax, C in zip(axes, [-1, 0, 3]): 
    for a, gamma in zip(ax, range(-1, 2)): 
        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a) 
axes[0, 0].legend(["class 0", "class 1", "sv class 0", "sv class 1"], 
                  ncol=4, loc=(.9, 1.2))





from sklearn.datasets import load_breast_cancer
X_train, X_test, y_train, y_test = train_test_split( 
    cancer.data, cancer.target, random_state=0) 
svc = SVC() 
svc.fit(X_train, y_train) 
print("Accuracy on training set: {:.2f}".format(svc.score(X_train, y_train))) 
print("Accuracy on test set: {:.2f}".format(svc.score(X_test, y_test)))





plt.plot(X_train.min(axis=0), 'o', label="min") 
plt.plot(X_train.max(axis=0), '^', label="max") 
plt.legend(loc=4) 
plt.xlabel("Feature index") 
plt.ylabel("Feature magnitude") 
plt.yscale("log")





# è®¡ç®—è®­ç»ƒé›†ä¸­æ¯ä¸ªç‰¹å¾çš„æœ€å°å€¼ 
min_on_training = X_train.min(axis=0) 
# è®¡ç®—è®­ç»ƒé›†ä¸­æ¯ä¸ªç‰¹å¾çš„èŒƒå›´ï¼ˆæœ€å¤§å€¼-æœ€å°å€¼ï¼‰ 
range_on_training = (X_train - min_on_training).max(axis=0) 
# å‡å»æœ€å°å€¼ï¼Œç„¶åé™¤ä»¥èŒƒå›´ 
# è¿™æ ·æ¯ä¸ªç‰¹å¾éƒ½æ˜¯min=0å’Œmax=1 
X_train_scaled = (X_train - min_on_training) / range_on_training 
print("Minimum for each feature\n{}".format(X_train_scaled.min(axis=0))) 
print("Maximum for each feature\n {}".format(X_train_scaled.max(axis=0)))





# åˆ©ç”¨è®­ç»ƒé›†çš„æœ€å°å€¼å’ŒèŒƒå›´å¯¹æµ‹è¯•é›†åšç›¸åŒçš„å˜æ¢ï¼ˆè¯¦è§ç¬¬3ç« ï¼‰ 
X_test_scaled = (X_test - min_on_training) / range_on_training


svc = SVC() 
svc.fit(X_train_scaled, y_train) 
print("Accuracy on training set: {:.3f}".format( 
    svc.score(X_train_scaled, y_train))) 
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))





svc = SVC(C=1000) 
svc.fit(X_train_scaled, y_train) 
print("Accuracy on training set: {:.3f}".format( 
    svc.score(X_train_scaled, y_train))) 
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))



