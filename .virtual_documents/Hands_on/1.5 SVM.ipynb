





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import mglearn
from sklearn.datasets import make_blobs
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix





from sklearn.tree import DecisionTreeClassifier 
cancer = load_breast_cancer() 
X_train, X_test, y_train, y_test = train_test_split( 
    cancer.data, cancer.target, stratify=cancer.target, random_state=42) 
tree = DecisionTreeClassifier(random_state=0) 
tree.fit(X_train, y_train) 
print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))


tree = DecisionTreeClassifier(max_depth=4, random_state=0) 
tree.fit(X_train, y_train)
print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train))) 
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))


from sklearn.tree import export_graphviz 
export_graphviz(tree, out_file="tree.dot", class_names=["malignant","benign"], 
                feature_names=cancer.feature_names, impurity=False, filled=True)
import graphviz 
with open("tree.dot") as f: 
    dot_graph = f.read() 
graphviz.Source(dot_graph)


def plot_feature_importances_cancer(model): 
    n_features = cancer.data.shape[1] 
    plt.barh(range(n_features), model.feature_importances_, align='center') 
    plt.yticks(np.arange(n_features), cancer.feature_names) 
    plt.xlabel("Feature importance") 
    plt.ylabel("Feature") 
plot_feature_importances_cancer(tree)


tree = mglearn.plots.plot_tree_not_monotone() 
display(tree)


# make_blobs 是来自 scikit-learn（即 sklearn）的一个用于生成聚类样本数据的函数
X, y = make_blobs(centers=4, random_state=8) 
y = y % 2 
mglearn.discrete_scatter(X[:, 0], X[:, 1], y) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")





from sklearn.svm import LinearSVC 
linear_svm = LinearSVC().fit(X, y) 
mglearn.plots.plot_2d_separator(linear_svm, X) 
mglearn.discrete_scatter(X[:, 0], X[:, 1], y) 
plt.xlabel("Feature 0") 
plt.ylabel("Feature 1")








%matplotlib inline  
# 在 Jupyter 中显示图像

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons      # 生成“月亮形”数据
from mpl_toolkits.mplot3d import Axes3D     # 3D 绘图支持
from sklearn.svm import SVC                 # SVM 模型

# 生成 40 个点，像两个月亮，加一点噪声让它更真实
X, y = make_moons(n_samples=40, noise=0.1, random_state=8)

# 查看一下数据长什么样
print("X shape:", X.shape)  # 应该是 (40, 2) —— 40 个点，每个有 2 个特征
print("First 5 points:\n", X[:5])
print("Labels:", y[:10])   # y 是 0 或 1





# 在二维空间中画出原始数据
plt.figure(figsize=(6, 4))
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0', edgecolors='k')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1', edgecolors='k')

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.legend()
plt.title("Original 2D Data (Not Linearly Separable)")
plt.grid(True)
plt.show()





# 创建新特征：x1 的平方
X_new = np.hstack([X, X[:, 1:] ** 2])

# 检查新数据形状
print("X_new shape:", X_new.shape)  # 应该是 (40, 3)
print("First few rows of X_new:\n", X_new[:3])





fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')  # 创建 3D 坐标系

# 分类绘制
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2],
           c='blue', s=80, label='Class 0', depthshade=False)

ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2],
           c='red', marker='^', s=80, label='Class 1', depthshade=False)

# 设置坐标轴标签
ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature 1 ** 2")

# 调整视角，看得更清楚
ax.view_init(elev=20, azim=30)

# 显示图例
plt.legend()
plt.title("3D View: After Adding Feature1 Squared")
plt.show()





# 使用线性 SVM（因为我们已经做了特征工程）
svm = SVC(kernel='linear', C=1000)  # C 很大表示不允许误分类（用于教学）
svm.fit(X_new, y)





# 创建网格点用于画平面
xx, yy = np.meshgrid(np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 10),
                     np.linspace(X_new[:, 1].min(), X_new[:, 1].max(), 10))
zz = (-svm.intercept_[0] - svm.coef_[0][0] * xx - svm.coef_[0][1] * yy) / svm.coef_[0][2]

# 画图
fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')

# 重新画点
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='blue', s=80, label='Class 0')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='red', marker='^', s=80, label='Class 1')

# 画决策平面
ax.plot_surface(xx, yy, zz, alpha=0.5, color='green', linewidth=0)

ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature 1 ** 2")
ax.view_init(elev=20, azim=30)
plt.legend()
plt.title("SVM Decision Plane in 3D Space")
plt.show()








# 生成干净数据
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 添加新特征
X_new = np.hstack([X, X[:, 1:] ** 2])

# 训练 SVM（高惩罚）
svm = SVC(kernel='linear', C=1e6)
svm.fit(X_new, y)

# 画图
fig = plt.figure(figsize=(9, 7))
ax = fig.add_subplot(111, projection='3d')

mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='blue', s=80, label='Class 0')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='red', marker='^', s=80, label='Class 1')

# 决策平面
xx, yy = np.meshgrid(np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 10),
                     np.linspace(X_new[:, 1].min(), X_new[:, 1].max(), 10))
zz = (-svm.intercept_[0] - svm.coef_[0][0] * xx - svm.coef_[0][1] * yy) / svm.coef_[0][2]
ax.plot_surface(xx, yy, zz, alpha=0.5, color='green')

ax.set_xlabel("Feature 0"); ax.set_ylabel("Feature 1"); ax.set_zlabel("Feature 1 ** 2")
ax.view_init(elev=20, azim=30)
plt.legend()
plt.title("Perfectly Separable in 3D Space")
plt.show()


%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.svm import SVC

# 1. 生成数据
X, y = make_moons(n_samples=100, noise=0.01, random_state=8)

# 2. 添加新特征：feature1 的平方
X_new = np.hstack([X, X[:, 1:] ** 2])

# 3. 训练 SVM
svm = SVC(kernel='linear', C=1e6)
svm.fit(X_new, y)

# 4. 在 2D 网格上预测（这才是正确的做法）
xx, yy = np.meshgrid(np.linspace(-2, 4, 200),
                     np.linspace(-1, 1.5, 200))
grid_points = np.c_[xx.ravel(), yy.ravel()]
grid_points_new = np.hstack([grid_points, grid_points[:, 1:] ** 2])
Z = svm.predict(grid_points_new).reshape(xx.shape)

# 5. 画图
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu_r)
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=60, label='Class 0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='^', s=60, label='Class 1')
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linewidths=2)

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("True Nonlinear Decision Boundary (via Prediction)")
plt.legend()
plt.grid(True)
plt.show()



