





mglearn.plots.plot_pca_illustration()








 fig, axes = plt.subplots(15, 2, figsize=(10, 20))
 malignant = cancer.data[cancer.target == 0]
 benign = cancer.data[cancer.target == 1]
 ax = axes.ravel()
 for i in range(30):
  _, bins = np.histogram(cancer.data[:, i], bins=50)
  ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)
  ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)
  ax[i].set_title(cancer.feature_names[i])
  ax[i].set_yticks(())
 ax[0].set_xlabel("Feature magnitude")
 ax[0].set_ylabel("Frequency")
 ax[0].legend(["malignant", "benign"], loc="best")
 fig.tight_layout()





 from sklearn.datasets import load_breast_cancer
 cancer = load_breast_cancer()
 scaler = StandardScaler()
 scaler.fit(cancer.data)
 X_scaled = scaler.transform(cancer.data)





 from sklearn.decomposition import PCA
 # keep the first two principal components of the data
 pca = PCA(n_components=2)
 # fit PCA model to breast cancer data
 pca.fit(X_scaled)
 # transform data onto the first two principal components
 X_pca = pca.transform(X_scaled)
 print("Original shape: {}".format(str(X_scaled.shape)))
 print("Reduced shape: {}".format(str(X_pca.shape)))





 # plot first vs. second principal component, colored by class
 plt.figure(figsize=(8, 8))
 mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)
 plt.legend(cancer.target_names, loc="best")
 plt.gca().set_aspect("equal")
 plt.xlabel("First principal component")
 plt.ylabel("Second principal component")








print("PCA component shape: {}".format(pca.components_.shape))


print("PCA components:\n{}".format(pca.components_))





 plt.matshow(pca.components_, cmap='viridis')
 plt.yticks([0, 1], ["First component", "Second component"])
 plt.colorbar()
 plt.xticks(range(len(cancer.feature_names)),
 cancer.feature_names, rotation=60, ha='left')
 plt.xlabel("Feature")
 plt.ylabel("Principal components")











 from sklearn.datasets import fetch_lfw_people
 people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
 image_shape = people.images[0].shape
 fix, axes = plt.subplots(2, 5, figsize=(15, 8),
 subplot_kw={'xticks': (), 'yticks': ()})
 for target, image, ax in zip(people.target, people.images, axes.ravel()):
  ax.imshow(image)
  ax.set_title(people.target_names[target])


 print("people.images.shape: {}".format(people.images.shape))
 print("Number of classes: {}".format(len(people.target_names)))








 # count how often each target appears
 counts = np.bincount(people.target)
 # print counts next to target names
 for i, (count, name) in enumerate(zip(counts, people.target_names)):
  print("{0:25} {1:3}".format(name, count), end='   ')
  if (i + 1) % 3 == 0:
   print()








 mask = np.zeros(people.target.shape, dtype=np.bool)
 for target in np.unique(people.target):
  mask[np.where(people.target == target)[0][:50]] = 1
 X_people = people.data[mask]
 y_people = people.target[mask]
 # scale the grayscale values to be between 0 and 1
 # instead of 0 and 255 for better numeric stability
 X_people = X_people / 255.














 from sklearn.neighbors import KNeighborsClassifier
 # split the data into training and test sets
 X_train, X_test, y_train, y_test = train_test_split(
  X_people, y_people, stratify=y_people, random_state=0)
 # build a KNeighborsClassifier using one neighbor
 knn = KNeighborsClassifier(n_neighbors=1)
 knn.fit(X_train, y_train)
 print("Test set score of 1-nn: {:.2f}".format(knn.score(X_test, y_test)))





mglearn.plots.plot_pca_whitening()





 pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)
 X_train_pca = pca.transform(X_train)
 X_test_pca = pca.transform(X_test)
 print("X_train_pca.shape: {}".format(X_train_pca.shape))





 knn = KNeighborsClassifier(n_neighbors=1)
 knn.fit(X_train_pca, y_train)
 print("Test set accuracy: {:.2f}".format(knn.score(X_test_pca, y_test)))






