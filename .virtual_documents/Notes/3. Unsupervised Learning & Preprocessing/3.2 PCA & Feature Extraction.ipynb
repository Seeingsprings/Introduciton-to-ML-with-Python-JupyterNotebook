


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import mglearn
from sklearn.datasets import make_blobs
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


mglearn.plots.plot_scaling()








 from sklearn.datasets import load_breast_cancer
 from sklearn.model_selection import train_test_split
 cancer = load_breast_cancer()
 X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
 random_state=1)
 print(X_train.shape)
 print(X_test.shape)





from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()





scaler.fit(X_train)





 # transform data
X_train_scaled = scaler.transform(X_train)
 # print dataset properties before and after scaling
np.set_printoptions(suppress=True, precision=4) # 删去这个就是科学计数法的表现形式
print("transformed shape: {}".format(X_train_scaled.shape))

print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0)))

print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))
 
print("per-feature minimum after scaling:\n {}".format(X_train_scaled.min(axis=0)))

print("per-feature maximum after scaling:\n {}".format(X_train_scaled.max(axis=0)))








 # transform test data
 X_test_scaled = scaler.transform(X_test)
 # print test data properties after scaling
 print("per-feature minimum after scaling:\n{}".format(X_test_scaled.min(axis=0)))
 print("per-feature maximum after scaling:\n{}".format(X_test_scaled.max(axis=0)))








 from sklearn.datasets import make_blobs

 # make synthetic data (构造数据)
 X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)

 # split it into training and test sets
 X_train, X_test = train_test_split(X, random_state=5, test_size=.1)

 # plot the training and test sets
 fig, axes = plt.subplots(1, 3, figsize=(13, 4))
 axes[0].scatter(X_train[:, 0], X_train[:, 1],
 c=mglearn.cm2(0), label="Training set", s=60)
 axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',
 c=mglearn.cm2(1), label="Test set", s=60)
 axes[0].legend(loc='upper left')
 axes[0].set_title("Original Data")
 
 # scale the data using MinMaxScaler
 scaler = MinMaxScaler()
 scaler.fit(X_train)
 X_train_scaled = scaler.transform(X_train)
 X_test_scaled = scaler.transform(X_test)

 # visualize the properly scaled data
 axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],
 c=mglearn.cm2(0), label="Training set", s=60)
 axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',
 c=mglearn.cm2(1), label="Test set", s=60)
 axes[1].set_title("Scaled Data")

 # rescale the test set separately（单独对测试集进行缩放）
 # so test set min is 0 and test set max is 1
 # DO NOT DO THIS! For illustration purposes only.
 test_scaler = MinMaxScaler()
 test_scaler.fit(X_test)
 X_test_scaled_badly = test_scaler.transform(X_test)

 # visualize wrongly scaled data
 axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],
 c=mglearn.cm2(0), label="training set", s=60)
 axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],
 marker='^', c=mglearn.cm2(1), label="test set", s=60)
 axes[2].set_title("Improperly Scaled Data")
 for ax in axes:
     ax.set_xlabel("Feature 0")
     ax.set_ylabel("Feature 1")











from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
 random_state=0)
svm = SVC(C=100,kernel='rbf') # default is rbf
svm.fit(X_train, y_train)
print("Test set accuracy: {:.2f}".format(svm.score(X_test, y_test)))





 # preprocessing using 0-1 scaling
 scaler = MinMaxScaler()
 scaler.fit(X_train)
 X_train_scaled = scaler.transform(X_train)
 X_test_scaled = scaler.transform(X_test)

 # learning an SVM on the scaled training data
 svm.fit(X_train_scaled, y_train)

 # scoring on the scaled test set
 print("Scaled test set accuracy: {:.2f}".format(
 svm.score(X_test_scaled, y_test)))








 # preprocessing using zero mean and unit variance scaling
 from sklearn.preprocessing import StandardScaler
 scaler = StandardScaler()
 scaler.fit(X_train)
 X_train_scaled = scaler.transform(X_train)
 X_test_scaled = scaler.transform(X_test)

 # learning an SVM on the scaled training data
 svm.fit(X_train_scaled, y_train)
 
 # scoring on the scaled test set
 print("SVM test accuracy: {:.2f}".format(svm.score(X_test_scaled, y_test)))





 from sklearn.datasets import load_breast_cancer
 cancer = load_breast_cancer()
 scaler = StandardScaler()
 scaler.fit(cancer.data)
 X_scaled = scaler.transform(cancer.data)





 from sklearn.decomposition import PCA
 # keep the first two principal components of the data
 pca = PCA(n_components=2)
 # fit PCA model to breast cancer data
 pca.fit(X_scaled)
 # transform data onto the first two principal components
 X_pca = pca.transform(X_scaled)
 print("Original shape: {}".format(str(X_scaled.shape)))
 print("Reduced shape: {}".format(str(X_pca.shape)))





 # plot first vs. second principal component, colored by class
 plt.figure(figsize=(8, 8))
 mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)
 plt.legend(cancer.target_names, loc="best")
 plt.gca().set_aspect("equal")
 plt.xlabel("First principal component")
 plt.ylabel("Second principal component")





print("PCA component shape: {}".format(pca.components_.shape))





print("PCA components:\n{}".format(pca.components_))











mglearn.plots.plot_pca_illustration()








 fig, axes = plt.subplots(15, 2, figsize=(10, 20))
 malignant = cancer.data[cancer.target == 0]
 benign = cancer.data[cancer.target == 1]
 ax = axes.ravel()
 for i in range(30):
  _, bins = np.histogram(cancer.data[:, i], bins=50)
  ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)
  ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)
  ax[i].set_title(cancer.feature_names[i])
  ax[i].set_yticks(())
 ax[0].set_xlabel("Feature magnitude")
 ax[0].set_ylabel("Frequency")
 ax[0].legend(["malignant", "benign"], loc="best")
 fig.tight_layout()





 from sklearn.datasets import load_breast_cancer
 cancer = load_breast_cancer()
 scaler = StandardScaler()
 scaler.fit(cancer.data)
 X_scaled = scaler.transform(cancer.data)





 from sklearn.decomposition import PCA
 # keep the first two principal components of the data
 pca = PCA(n_components=2)
 # fit PCA model to breast cancer data
 pca.fit(X_scaled)
 # transform data onto the first two principal components
 X_pca = pca.transform(X_scaled)
 print("Original shape: {}".format(str(X_scaled.shape)))
 print("Reduced shape: {}".format(str(X_pca.shape)))





 # plot first vs. second principal component, colored by class
 plt.figure(figsize=(8, 8))
 mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)
 plt.legend(cancer.target_names, loc="best")
 plt.gca().set_aspect("equal")
 plt.xlabel("First principal component")
 plt.ylabel("Second principal component")








 plt.matshow(pca.components_, cmap='viridis')
 plt.yticks([0, 1], ["First component", "Second component"])
 plt.colorbar()
 plt.xticks(range(len(cancer.feature_names)),
 cancer.feature_names, rotation=60, ha='left')
 plt.xlabel("Feature")
 plt.ylabel("Principal components")











 from sklearn.datasets import fetch_lfw_people
 people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
 image_shape = people.images[0].shape
 fix, axes = plt.subplots(2, 5, figsize=(15, 8),
 subplot_kw={'xticks': (), 'yticks': ()})
 for target, image, ax in zip(people.target, people.images, axes.ravel()):
  ax.imshow(image)
  ax.set_title(people.target_names[target])


 print("people.images.shape: {}".format(people.images.shape))
 print("Number of classes: {}".format(len(people.target_names)))








 # count how often each target appears
 counts = np.bincount(people.target)
 # print counts next to target names
 for i, (count, name) in enumerate(zip(counts, people.target_names)):
  print("{0:25} {1:3}".format(name, count), end='   ')
  if (i + 1) % 3 == 0:
   print()








 mask = np.zeros(people.target.shape, dtype=np.bool)
 for target in np.unique(people.target):
  mask[np.where(people.target == target)[0][:50]] = 1
 X_people = people.data[mask]
 y_people = people.target[mask]
 # scale the grayscale values to be between 0 and 1
 # instead of 0 and 255 for better numeric stability
 X_people = X_people / 255.














 from sklearn.neighbors import KNeighborsClassifier
 # split the data into training and test sets
 X_train, X_test, y_train, y_test = train_test_split(
  X_people, y_people, stratify=y_people, random_state=0)
 # build a KNeighborsClassifier using one neighbor
 knn = KNeighborsClassifier(n_neighbors=1)
 knn.fit(X_train, y_train)
 print("Test set score of 1-nn: {:.2f}".format(knn.score(X_test, y_test)))





mglearn.plots.plot_pca_whitening()





 pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)
 X_train_pca = pca.transform(X_train)
 X_test_pca = pca.transform(X_test)
 print("X_train_pca.shape: {}".format(X_train_pca.shape))





knn = KNeighborsClassifier(n_neighbors=1) 
knn.fit(X_train_pca, y_train) 
print("Test set accuracy: {:.2f}".format(knn.score(X_test_pca, y_test)))





 print("pca.components_.shape: {}".format(pca.components_.shape))


 fix, axes = plt.subplots(3, 5, figsize=(15, 12),
 subplot_kw={'xticks': (), 'yticks': ()})
 for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):
  ax.imshow(component.reshape(image_shape),
  cmap='viridis')
  ax.set_title("{}. component".format((i + 1)))





 mglearn.plots.plot_pca_faces(X_train, X_test, image_shape)








mglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train) 
plt.xlabel("First principal component") 
plt.ylabel("Second principal component")








# Step 1: Import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

# Step 2: Load data
print("Loading LFW people dataset...")
people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
image_shape = people.images[0].shape

# Print basic info
print(f"Number of samples: {len(people.images)}")
print(f"Image shape: {image_shape}")
print(f"Number of classes: {len(people.target_names)}")

# Step 3: Limit to 50 images per person
X = people.data
y = people.target

# Create mask: keep at most 50 images per person
mask = np.zeros(y.shape, dtype=np.bool_)
for target in np.unique(y):
    mask[np.where(y == target)[0][:50]] = 1

X = X[mask]
y = y[mask]

# Normalize pixel values to [0, 1]
X = X / 255.

# Step 4: Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, random_state=0)

# Step 5: Apply PCA with whitening
print("Applying PCA...")
pca = PCA(n_components=100, whiten=True, random_state=0)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

print(f"X_train_pca shape: {X_train_pca.shape}")

# Step 6: Train KNN
print("Training KNN classifier...")
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train_pca, y_train)

# Evaluate
accuracy = knn.score(X_test_pca, y_test)
print(f"Test set accuracy: {accuracy:.2f}")  # Should be around 0.36


from sklearn.datasets import fetch_lfw_people
people = fetch_lfw_people(min_faces_per_person=20, resize=0.7, download_if_missing=True)


# To plot eigenfaces
fig, axes = plt.subplots(2, 5, figsize=(15, 8))
for i, ax in enumerate(axes.ravel()):
    # Reshape component and scale for display
    component = pca.components_[i].reshape(image_shape)
    ax.imshow(component, cmap='viridis')
    ax.set_title(f"Component {i+1}")
plt.show()


# From earlier steps
pca = PCA(n_components=500, whiten=True, random_state=0).fit(X_train)  # ← Use 500 components!
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# Pick 3 sample faces (first 3 in test set)
sample_indices = [0, 1, 2]  # or any 3 indices you like


import matplotlib.pyplot as plt

# Define number of components to show
n_components_list = [10, 50, 100, 500]
titles = ['Original Image'] + [f'{n} components' for n in n_components_list]

# Create subplot grid: 3 rows (faces), 5 columns (original + 4 reconstructions)
fig, axes = plt.subplots(3, 5, figsize=(15, 9))

for i, idx in enumerate(sample_indices):
    # Original face
    original = X_test[idx].reshape(image_shape)
    axes[i, 0].imshow(original, cmap='gray')
    axes[i, 0].set_title(titles[0])
    axes[i, 0].axis('off')

    # Reconstruct with different numbers of components
    for j, n_comp in enumerate(n_components_list):
        # Create new PCA with n_comp components
        pca_temp = PCA(n_components=n_comp, whiten=True, random_state=0)
        pca_temp.fit(X_train)  # Fit on training data
        X_test_pca_temp = pca_temp.transform(X_test)  # Transform test data
        X_reconstructed = pca_temp.inverse_transform(X_test_pca_temp[idx:idx+1]).reshape(image_shape)

        # Rescale for visualization
        X_reconstructed = (X_reconstructed - X_reconstructed.min()) / (X_reconstructed.max() - X_reconstructed.min())

        axes[i, j+1].imshow(X_reconstructed, cmap='gray')
        axes[i, j+1].set_title(titles[j+1])
        axes[i, j+1].axis('off')

plt.tight_layout()
plt.show()








mglearn.plots.plot_nmf_illustration()








mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)





from sklearn.decomposition import NMF 
nmf = NMF(n_components=15, random_state=0) 
nmf.fit(X_train) 
X_train_nmf = nmf.transform(X_train) 
X_test_nmf = nmf.transform(X_test) 
fix, axes = plt.subplots(3, 5, figsize=(15, 12), 
                         subplot_kw={'xticks': (), 'yticks': ()}) 
for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())): 
    ax.imshow(component.reshape(image_shape)) 
    ax.set_title("{}. component".format(i))






