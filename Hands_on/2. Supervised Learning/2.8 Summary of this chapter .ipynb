{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dde7241-d080-47fa-9fce-4028a2ce32bb",
   "metadata": {},
   "source": [
    "# 🔍 Part 1: Recap of Key Concepts\n",
    "\n",
    "### ✅ What You Learned:\n",
    "\n",
    "#### 1️⃣ **Model Complexity & Generalization**\n",
    "\n",
    "- **Underfitting (欠拟合)**: Model is too simple → can’t capture patterns in training data.\n",
    "- **Overfitting (过拟合)**: Model is too complex → memorizes training data, fails on new data.\n",
    "- **Generalization**: Ability to perform well on *unseen* data — this is the goal!\n",
    "\n",
    "> 💡 The sweet spot is a model that’s complex enough to learn, but not so complex that it overfits.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2️⃣ **Model Selection & Tuning**\n",
    "\n",
    "You explored many algorithms:\n",
    "\n",
    "- Linear models\n",
    "- Naive Bayes\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "- SVM\n",
    "- Neural Networks\n",
    "\n",
    "Each has strengths/weaknesses depending on:\n",
    "\n",
    "- Data size\n",
    "- Feature scaling needs\n",
    "- Interpretability\n",
    "- Speed\n",
    "- Parameter sensitivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0531ed5-280b-47e0-a079-3f7792a317a7",
   "metadata": {},
   "source": [
    "# 🧭 Part 2: Quick Reference Guide — When to Use Which Model?\n",
    "| Model Type | Best For | Pros | Cons |\n",
    "| --- | --- | --- | --- |\n",
    "| Nearest Neighbors | Small datasets, baselines | Simple, no training | Slow on big data, sensitive to scale |\n",
    "| Linear Models | Large/high-dim data, interpretability | Fast, scalable, interpretable | Assumes linearity |\n",
    "| Naive Bayes | Text, fast classification | Super fast, low data need | Assumes independence |\n",
    "| Decision Tree | Explainability, quick prototyping | Visual, no scaling | Overfits easily |\n",
    "| Random Forest | All-around strong performer | Robust, handles noise, no scaling needed | Less interpretable, slower than tree |\n",
    "| Gradient Boosting | High accuracy, production systems | Top performance, fast prediction | Needs tuning, slow training |\n",
    "| SVM | Medium data, non-linear boundaries | Strong generalization | Slow, needs scaling, hard to tune |\n",
    "| Neural Networks | Big data, complex patterns (images/NLP) | Most powerful for deep learning | Black box, needs lots of data & compute |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc277d-2e05-4be9-8f75-23a3c6474208",
   "metadata": {},
   "source": [
    "# 📈 Part 3: Practical Advice — How to Start Building Models\n",
    "\n",
    "> “面对新数据集，通常最好先从简单模型开始...”\n",
    "\n",
    "### ✅ Step-by-Step Strategy:\n",
    "\n",
    "1. **Start Simple**  \n",
    "\t→ Try `Linear Model`, `Naive Bayes`, or `Nearest Neighbors` first.  \n",
    "\t→ Get a baseline performance quickly.\n",
    "2. **Understand Your Data**  \n",
    "\t→ Check feature scales, missing values, class imbalance.  \n",
    "\t→ Plot distributions, correlations.\n",
    "3. **Move to Complex Models**  \n",
    "\t→ If simple models underperform → try `Random Forest`, `Gradient Boosting`, `SVM`, or `Neural Network`.\n",
    "4. **Tune Parameters Carefully**  \n",
    "\t→ Don’t guess — use grid search, random search, or automated tools (coming in Chapter 6).\n",
    "5. **Test on Realistic Data**  \n",
    "\t→ Always evaluate on held-out test set or cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hands-on)",
   "language": "python",
   "name": "hands-on-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
